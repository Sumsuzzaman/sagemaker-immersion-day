{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the proper libraries needed\n",
    "!pip install Pillow\n",
    "!pip install matplotlib\n",
    "!pip install py2neo==2.0\n",
    "!pip install boto3\n",
    "!pip install jgraph\n",
    "\n",
    "from PIL import Image\n",
    "import webbrowser\n",
    "import datetime\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "from pprint import pprint\n",
    "import boto3\n",
    "import jgraph\n",
    "from py2neo import Graph, authenticate, Relationship\n",
    "import urllib\n",
    "from scripts.rekgraph import rekrelationships\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve and print a selected image\n",
    "#use the same file name for each temp image or you will start getting a ton of photos\n",
    "#stored in the folder\n",
    "# urllib.urlretrieve(\"https://images-na.ssl-images-amazon.com/images/M/MV5BNDgwNjI5MzY3Ml5BMl5BanBnXkFtZTgwNDE5NTAzMTI@._V1_SY1000_CR0,0,674,1000_AL_.jpg\", \"images/image1.jpg\")                                                               \n",
    "img_path = 'images/forest_gump.jpg'\n",
    "imshow(np.asarray(Image.open(img_path, 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect faces in the first picture in the image\n",
    "client = boto3.client('rekognition', region_name='us-east-1')\n",
    "response = client.detect_faces(\n",
    "    Image={\n",
    "        'Bytes': open(img_path,'rb').read().strip()\n",
    "    }\n",
    ")\n",
    "#the api returns a json dictionary of length 3\n",
    "print('data type and length of the response from the detect_faces call')\n",
    "print(type(response))\n",
    "print(len(response))\n",
    "\n",
    "#print how many faces are in this picture\n",
    "print('the number of faces detected in the picture')\n",
    "faceCount=len(response['FaceDetails'])\n",
    "print(faceCount)\n",
    " \n",
    "if(faceCount>0):\n",
    "    #next step is to show only the cropped faces\n",
    "    #the IndexFaces api call will automatically take care of this parsing\n",
    "    #for you when sending pictures with multiple faces to a FaceCollection.\n",
    "    print('printing face bounding boxes below')\n",
    "    counter=0\n",
    "    #which of the faces from the photo do you want to print? rekognition makes the\n",
    "    #largest face equal to index zero\n",
    "    faceNumToKeep=0\n",
    "    #these toKeep____ variables are to give flexibility\n",
    "    #on which face to crop and print\n",
    "    toKeepLeft=0.0\n",
    "    toKeepTop=0.0\n",
    "    toKeepWidth=0.0\n",
    "    toKeepHeight=0.0\n",
    "\n",
    "    #loop over each face and print the bounding boxes, save the one\n",
    "    #specified by the faceNumToKeep param\n",
    "    #for pictures with multiple faces, Rekognition starts with the largest face and moves iteratively to smaller ones\n",
    "    for x in response['FaceDetails']:\n",
    "        print('\\nface number '+str(counter))\n",
    "        boundingBox=x['BoundingBox']\n",
    "        #print(boundingBox)\n",
    "        print(boundingBox['Left'])\n",
    "        print(boundingBox['Top'])\n",
    "        print(boundingBox['Width'])\n",
    "        print(boundingBox['Height'])\n",
    "        if (counter==faceNumToKeep):\n",
    "            toKeepLeft=boundingBox['Left']\n",
    "            toKeepTop=boundingBox['Top']\n",
    "            toKeepWidth=boundingBox['Width']\n",
    "            toKeepHeight=boundingBox['Height']\n",
    "        counter=counter+1\n",
    "   \n",
    "    #retrieve the whole photo, crop the face of interest and print                                                                 \n",
    "    pil_im = Image.open(img_path, 'r')\n",
    "   \n",
    "    picwidth=pil_im.size[0]\n",
    "    picheight=pil_im.size[1]\n",
    "    print('width '+str(picwidth))\n",
    "    print('height '+str(picheight))\n",
    "    imshow(np.asarray(pil_im.crop((\n",
    "        picwidth*toKeepLeft,\n",
    "        picheight*toKeepTop,\n",
    "        picwidth*toKeepLeft+picwidth*toKeepWidth,\n",
    "        picheight*toKeepTop+picheight*toKeepHeight\n",
    "                             ))))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect celebrities\n",
    "response = client.recognize_celebrities(\n",
    "    Image={\n",
    "        'Bytes': open(img_path,'rb').read().strip()\n",
    "    }\n",
    ")\n",
    "\n",
    "print ('num celebrities found: '+str(len(response['CelebrityFaces'])))\n",
    "for i in response['CelebrityFaces']:\n",
    "    print i['Name']\n",
    "    imageName = i['Name']\n",
    "print response['CelebrityFaces']\n",
    "name1=response['CelebrityFaces'][0]['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_chainer_p27",
   "language": "python",
   "name": "conda_chainer_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
